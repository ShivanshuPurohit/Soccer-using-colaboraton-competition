{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./python\n",
      "Collecting tensorflow==1.7.1 (from unityagents==0.4.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/83/35c3f53129dfc80d65ebbe07ef0575263c3c05cc37f8c713674dcedcea6f/tensorflow-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (48.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 48.1MB 925kB/s eta 0:00:01   19% |██████▍                         | 9.6MB 26.7MB/s eta 0:00:02    43% |██████████████                  | 21.0MB 29.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=4.2.1 in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (5.2.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (1.12.1)\n",
      "Collecting jupyter (from unityagents==0.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/df/0f5dd132200728a86190397e1ea87cd76244e42d39ec5e88efd25b2abd7e/jupyter-1.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pytest>=3.2.2 in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (4.5.0)\n",
      "Collecting docopt (from unityagents==0.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (3.12)\n",
      "Collecting protobuf==3.5.2 (from unityagents==0.4.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/0d/6e9cf54be86c742a375a0ca0addadb582c890939b1b6d3339570e51091bd/protobuf-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (6.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.4MB 6.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio==1.11.0 (from unityagents==0.4.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/b8/00e703183b7ae5e02f161dafacdfa8edbd7234cb7434aef00f126a3a511e/grpcio-1.11.0-cp36-cp36m-manylinux1_x86_64.whl (8.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 8.8MB 6.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch==0.4.0 in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (0.23.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (1.2.1)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.6/site-packages (from unityagents==0.4.0) (4.9.0)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow==1.7.1->unityagents==0.4.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 30.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.8.0,>=1.7.0 (from tensorflow==1.7.1->unityagents==0.4.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/ec/65d4e8410038ca2a78c09034094403d231228d0ddcae7d470b223456e55d/tensorboard-1.7.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 11.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.7.1->unityagents==0.4.0) (1.11.0)\n",
      "Collecting gast>=0.2.0 (from tensorflow==1.7.1->unityagents==0.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow==1.7.1->unityagents==0.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.7.1->unityagents==0.4.0) (0.30.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==1.7.1->unityagents==0.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: python-dateutil>=2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->unityagents==0.4.0) (2.6.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from matplotlib->unityagents==0.4.0) (2017.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib->unityagents==0.4.0) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->unityagents==0.4.0) (2.2.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.6/site-packages (from jupyter->unityagents==0.4.0) (7.0.5)\n",
      "Collecting qtconsole (from jupyter->unityagents==0.4.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/9c/ee26b844381f0cf2ea24bd822e4a9ed2c7fd6d8cdeef63be459c62132f9b/qtconsole-4.7.4-py2.py3-none-any.whl (118kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 28.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nbconvert in /opt/conda/lib/python3.6/site-packages (from jupyter->unityagents==0.4.0) (5.4.0)\n",
      "Collecting jupyter-console (from jupyter->unityagents==0.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/89/742fa5a80b552ffcb6a8922712697c6e6828aee7b91ee4ae2b79f00f8401/jupyter_console-6.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.6/site-packages (from jupyter->unityagents==0.4.0) (5.7.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (38.4.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (1.8.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /opt/conda/lib/python3.6/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (0.1.7)\n",
      "Requirement already satisfied: pluggy!=0.10,<1.0,>=0.9 in /opt/conda/lib/python3.6/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (0.11.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0; python_version > \"2.7\" in /opt/conda/lib/python3.6/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (7.0.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from pytest>=3.2.2->unityagents==0.4.0) (19.1.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from ipykernel->unityagents==0.4.0) (6.5.0)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /opt/conda/lib/python3.6/site-packages (from ipykernel->unityagents==0.4.0) (4.3.2)\n",
      "Requirement already satisfied: jupyter_client in /opt/conda/lib/python3.6/site-packages (from ipykernel->unityagents==0.4.0) (5.2.4)\n",
      "Requirement already satisfied: tornado>=4.0 in /opt/conda/lib/python3.6/site-packages (from ipykernel->unityagents==0.4.0) (4.5.3)\n",
      "Requirement already satisfied: bleach==1.5.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents==0.4.0) (1.5.0)\n",
      "Requirement already satisfied: html5lib==0.9999999 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents==0.4.0) (0.9999999)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents==0.4.0) (2.6.9)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.1->unityagents==0.4.0) (0.14.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets->jupyter->unityagents==0.4.0) (4.4.0)\n",
      "Collecting widgetsnbextension~=3.0.0 (from ipywidgets->jupyter->unityagents==0.4.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/f2/c8bcccccbed39d51d3e237fb0c0f0c9bbc845d12afc41f5ca5f5728fffc7/widgetsnbextension-3.0.8-py2.py3-none-any.whl (2.2MB)\n",
      "\u001b[K    93% |█████████████████████████████▉  | 2.0MB 32.7MB/s eta 0:00:01\u001b[K    100% |████████████████████████████████| 2.2MB 12.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from qtconsole->jupyter->unityagents==0.4.0) (2.2.0)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.6/site-packages (from qtconsole->jupyter->unityagents==0.4.0) (4.4.0)\n",
      "Requirement already satisfied: pyzmq>=17.1 in /opt/conda/lib/python3.6/site-packages (from qtconsole->jupyter->unityagents==0.4.0) (17.1.2)\n",
      "Collecting qtpy (from qtconsole->jupyter->unityagents==0.4.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/fd/9972948f02e967b691cc0ca1f26124826a3b88cb38f412a8b7935b8c3c72/QtPy-1.9.0-py2.py3-none-any.whl (54kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 23.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipython-genutils in /opt/conda/lib/python3.6/site-packages (from qtconsole->jupyter->unityagents==0.4.0) (0.2.0)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /opt/conda/lib/python3.6/site-packages (from nbconvert->jupyter->unityagents==0.4.0) (0.8.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.6/site-packages (from nbconvert->jupyter->unityagents==0.4.0) (2.10)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from nbconvert->jupyter->unityagents==0.4.0) (0.2.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.6/site-packages (from nbconvert->jupyter->unityagents==0.4.0) (1.4.1)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.6/site-packages (from nbconvert->jupyter->unityagents==0.4.0) (0.3.1)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.6/site-packages (from nbconvert->jupyter->unityagents==0.4.0) (0.5.0)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 (from jupyter-console->jupyter->unityagents==0.4.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/a7/81b39aa50e9284fe2cb21cc7fb7de7817b224172d42793fd57451d38842b/prompt_toolkit-3.0.5-py3-none-any.whl (351kB)\n",
      "\u001b[K    100% |████████████████████████████████| 358kB 20.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Send2Trash in /opt/conda/lib/python3.6/site-packages (from notebook->jupyter->unityagents==0.4.0) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /opt/conda/lib/python3.6/site-packages (from notebook->jupyter->unityagents==0.4.0) (0.8.1)\n",
      "Requirement already satisfied: prometheus_client in /opt/conda/lib/python3.6/site-packages (from notebook->jupyter->unityagents==0.4.0) (0.3.1)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->unityagents==0.4.0) (0.8.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->unityagents==0.4.0) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->unityagents==0.4.0) (4.3.1)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->unityagents==0.4.0) (0.10.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->unityagents==0.4.0) (4.0.11)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->unityagents==0.4.0) (0.7.4)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->unityagents==0.4.0) (2.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from jinja2->nbconvert->jupyter->unityagents==0.4.0) (1.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel->unityagents==0.4.0) (0.5.2)\n",
      "Building wheels for collected packages: unityagents, docopt, absl-py, termcolor\n",
      "  Running setup.py bdist_wheel for unityagents ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-h4h06los/wheels/97/7a/24/09937717b9737178ae827bcef33ba219b540efd55be210010c\n",
      "  Running setup.py bdist_wheel for docopt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built unityagents docopt absl-py termcolor\n",
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.5 which is incompatible.\u001b[0m\n",
      "Installing collected packages: absl-py, protobuf, tensorboard, gast, astor, termcolor, grpcio, tensorflow, qtpy, qtconsole, prompt-toolkit, jupyter-console, jupyter, docopt, unityagents, widgetsnbextension\n",
      "  Found existing installation: protobuf 3.5.1\n",
      "    Uninstalling protobuf-3.5.1:\n",
      "      Successfully uninstalled protobuf-3.5.1\n",
      "  Found existing installation: tensorflow 1.3.0\n",
      "    Uninstalling tensorflow-1.3.0:\n",
      "      Successfully uninstalled tensorflow-1.3.0\n",
      "  Found existing installation: prompt-toolkit 1.0.15\n",
      "    Uninstalling prompt-toolkit-1.0.15:\n",
      "      Successfully uninstalled prompt-toolkit-1.0.15\n",
      "  Found existing installation: widgetsnbextension 3.1.0\n",
      "    Uninstalling widgetsnbextension-3.1.0:\n",
      "      Successfully uninstalled widgetsnbextension-3.1.0\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 docopt-0.6.2 gast-0.3.3 grpcio-1.11.0 jupyter-1.0.0 jupyter-console-6.1.0 prompt-toolkit-3.0.5 protobuf-3.5.2 qtconsole-4.7.4 qtpy-1.9.0 tensorboard-1.7.0 tensorflow-1.7.1 termcolor-1.1.0 unityagents-0.4.0 widgetsnbextension-3.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 2\n",
      "        Number of External Brains : 2\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: GoalieBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 112\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n",
      "Unity brain name: StrikerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 112\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 6\n",
      "        Vector Action descriptions: , , , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GoalieBrain', 'StrikerBrain']\n",
      "Number of goalie agents: 2\n",
      "Number of striker agents: 2\n",
      "Number of goalie actions: 4\n",
      "Number of striker actions: 6\n",
      "There are 2 goalie agents. Each receives a state with length: 336\n",
      "There are 2 striker agents. Each receives a state with length: 336\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from soccer_model import ActorModel, CriticModel\n",
    "from memory import Memory\n",
    "\n",
    "from soccer_agent import Agent\n",
    "from optimizer import Optimizer\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# environment configuration\n",
    "env = UnityEnvironment(file_name=\"Soccer_Windows_x86_64/Soccer.exe\", seed=1)\n",
    "\n",
    "# print the brain names\n",
    "print(env.brain_names)\n",
    "\n",
    "# set the goalie brain\n",
    "g_brain_name = env.brain_names[0]\n",
    "g_brain = env.brains[g_brain_name]\n",
    "\n",
    "# set the striker brain\n",
    "s_brain_name = env.brain_names[1]\n",
    "s_brain = env.brains[s_brain_name]\n",
    "\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)\n",
    "\n",
    "# number of agents \n",
    "n_goalie_agents = len(env_info[g_brain_name].agents)\n",
    "print('Number of goalie agents:', n_goalie_agents)\n",
    "n_striker_agents = len(env_info[s_brain_name].agents)\n",
    "print('Number of striker agents:', n_striker_agents)\n",
    "\n",
    "# number of actions\n",
    "goalie_action_size = g_brain.vector_action_space_size\n",
    "print('Number of goalie actions:', goalie_action_size)\n",
    "striker_action_size = s_brain.vector_action_space_size\n",
    "print('Number of striker actions:', striker_action_size)\n",
    "\n",
    "# examine the state space \n",
    "goalie_states = env_info[g_brain_name].vector_observations\n",
    "goalie_state_size = goalie_states.shape[1]\n",
    "print('There are {} goalie agents. Each receives a state with length: {}'.format(goalie_states.shape[0], goalie_state_size))\n",
    "striker_states = env_info[s_brain_name].vector_observations\n",
    "striker_state_size = striker_states.shape[1]\n",
    "print('There are {} striker agents. Each receives a state with length: {}'.format(striker_states.shape[0], striker_state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "N_STEP = 8\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.995\n",
    "EPSILON = 0.1\n",
    "ENTROPY_WEIGHT = 0.001\n",
    "GRADIENT_CLIP = 0.5\n",
    "GOALIE_LR = 8e-5\n",
    "STRIKER_LR = 1e-4\n",
    "\n",
    "\n",
    "CHECKPOINT_GOALIE_ACTOR = './checkpoint_goalie_actor.pth'\n",
    "CHECKPOINT_GOALIE_CRITIC = './checkpoint_goalie_critic.pth'\n",
    "CHECKPOINT_STRIKER_ACTOR = './checkpoint_striker_actor.pth'\n",
    "CHECKPOINT_STRIKER_CRITIC = './checkpoint_striker_critic.pth'\n",
    "\n",
    "# Actors and Critics\n",
    "GOALIE_0_KEY = 0\n",
    "STRIKER_0_KEY = 0\n",
    "GOALIE_1_KEY = 1\n",
    "STRIKER_1_KEY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURAL MODEL\n",
    "goalie_actor_model = ActorModel( goalie_state_size, goalie_action_size ).to(DEVICE)\n",
    "goalie_critic_model = CriticModel( goalie_state_size + striker_state_size + goalie_state_size + striker_state_size ).to(DEVICE)\n",
    "goalie_optim = optim.Adam( list( goalie_actor_model.parameters() ) + list( goalie_critic_model.parameters() ), lr=GOALIE_LR )\n",
    "# self.optim = optim.RMSprop( list( self.actor_model.parameters() ) + list( self.critic_model.parameters() ), lr=lr, alpha=0.99, eps=1e-5 )\n",
    "\n",
    "\n",
    "striker_actor_model = ActorModel( striker_state_size, striker_action_size ).to(DEVICE)\n",
    "striker_critic_model = CriticModel( striker_state_size + goalie_state_size + striker_state_size + goalie_state_size ).to(DEVICE)\n",
    "striker_optim = optim.Adam( list( striker_actor_model.parameters() ) + list( striker_critic_model.parameters() ), lr=STRIKER_LR )\n",
    "# self.optim = optim.RMSprop( list( self.actor_model.parameters() ) + list( self.critic_model.parameters() ), lr=lr, alpha=0.99, eps=1e-5 )\n",
    "\n",
    "goalie_actor_model.load( CHECKPOINT_GOALIE_ACTOR )\n",
    "goalie_critic_model.load( CHECKPOINT_GOALIE_CRITIC )\n",
    "striker_actor_model.load( CHECKPOINT_STRIKER_ACTOR )\n",
    "striker_critic_model.load( CHECKPOINT_STRIKER_CRITIC )\n",
    "\n",
    "\n",
    "# AGENTS\n",
    "goalie_0 = Agent( DEVICE, GOALIE_0_KEY, goalie_actor_model, N_STEP )\n",
    "goalie_optimizer = Optimizer( DEVICE, goalie_actor_model, goalie_critic_model, goalie_optim,  \n",
    "    N_STEP, BATCH_SIZE, GAMMA, EPSILON, ENTROPY_WEIGHT, GRADIENT_CLIP)\n",
    "\n",
    "striker_0 = Agent( DEVICE, STRIKER_0_KEY, striker_actor_model, N_STEP )\n",
    "striker_optimizer = Optimizer( DEVICE, striker_actor_model, striker_critic_model, striker_optim,  \n",
    "    N_STEP, BATCH_SIZE, GAMMA, EPSILON, ENTROPY_WEIGHT, GRADIENT_CLIP)\n",
    "\n",
    "goalie_1 = Agent( DEVICE, GOALIE_1_KEY, goalie_actor_model, N_STEP )\n",
    "goalie_optimizer = Optimizer( DEVICE, goalie_actor_model, goalie_critic_model, goalie_optim,  \n",
    "    N_STEP, BATCH_SIZE, GAMMA, EPSILON, ENTROPY_WEIGHT, GRADIENT_CLIP)\n",
    "\n",
    "striker_1 = Agent( DEVICE, STRIKER_1_KEY, striker_actor_model, N_STEP )\n",
    "striker_optimizer = Optimizer( DEVICE, striker_actor_model, striker_critic_model, striker_optim,  \n",
    "    N_STEP, BATCH_SIZE, GAMMA, EPSILON, ENTROPY_WEIGHT, GRADIENT_CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_train():\n",
    "    n_episodes = 5000\n",
    "    team_0_window_score = deque(maxlen=100)\n",
    "    team_0_window_score_wins = deque(maxlen=100)\n",
    "\n",
    "    team_1_window_score = deque(maxlen=100)\n",
    "    team_1_window_score_wins = deque(maxlen=100)\n",
    "\n",
    "    draws = deque(maxlen=100)\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        env_info = env.reset(train_mode=True)                        # reset the environment    \n",
    "\n",
    "        goalies_states = env_info[g_brain_name].vector_observations  # get initial state (goalies)\n",
    "        strikers_states = env_info[s_brain_name].vector_observations # get initial state (strikers)\n",
    "\n",
    "        goalies_scores = np.zeros(n_goalie_agents)                   # initialize the score (goalies)\n",
    "        strikers_scores = np.zeros(n_striker_agents)                 # initialize the score (strikers)         \n",
    "\n",
    "        steps = 0\n",
    "        \n",
    "        while True:       \n",
    "            # select actions and send to environment\n",
    "            action_goalie_0, log_prob_goalie_0 = goalie_0.act( goalies_states[goalie_0.KEY] )\n",
    "            action_striker_0, log_prob_striker_0 = striker_0.act( strikers_states[striker_0.KEY] )\n",
    "\n",
    "            # action_goalie_1, log_prob_goalie_1 = goalie_1.act( goalies_states[goalie_1.KEY] )\n",
    "            # action_striker_1, log_prob_striker_1 = striker_1.act( strikers_states[striker_1.KEY] )\n",
    "            \n",
    "            # random            \n",
    "            action_goalie_1 = np.asarray( [np.random.choice(goalie_action_size)] )\n",
    "            action_striker_1 = np.asarray( [np.random.choice(striker_action_size)] )\n",
    "\n",
    "\n",
    "            actions_goalies = np.array( (action_goalie_0, action_goalie_1) )                                    \n",
    "            actions_strikers = np.array( (action_striker_0, action_striker_1) )\n",
    "\n",
    "            actions = dict( zip( [g_brain_name, s_brain_name], [actions_goalies, actions_strikers] ) )\n",
    "\n",
    "        \n",
    "            env_info = env.step(actions)                                                \n",
    "            # get next states\n",
    "            goalies_next_states = env_info[g_brain_name].vector_observations         \n",
    "            strikers_next_states = env_info[s_brain_name].vector_observations\n",
    "            \n",
    "            # get reward and update scores\n",
    "            goalies_rewards = env_info[g_brain_name].rewards  \n",
    "            strikers_rewards = env_info[s_brain_name].rewards\n",
    "            goalies_scores += goalies_rewards\n",
    "            strikers_scores += strikers_rewards\n",
    "                        \n",
    "            # check if episode finished\n",
    "            done = np.any(env_info[g_brain_name].local_done)\n",
    "\n",
    "            # store experiences\n",
    "            goalie_0_reward = goalies_rewards[goalie_0.KEY]\n",
    "            goalie_0.step( \n",
    "                goalies_states[goalie_0.KEY],\n",
    "                np.concatenate( \n",
    "                    (\n",
    "                        goalies_states[goalie_0.KEY],\n",
    "                        strikers_states[striker_0.KEY],\n",
    "                        goalies_states[GOALIE_1_KEY],\n",
    "                        strikers_states[STRIKER_1_KEY],\n",
    "                    ), axis=0 ),\n",
    "                action_goalie_0,\n",
    "                log_prob_goalie_0,\n",
    "                goalie_0_reward \n",
    "            )\n",
    "\n",
    "\n",
    "            striker_0_reward = strikers_rewards[striker_0.KEY]\n",
    "            striker_0.step(                 \n",
    "                strikers_states[striker_0.KEY],\n",
    "                np.concatenate( \n",
    "                    (\n",
    "                        strikers_states[striker_0.KEY],\n",
    "                        goalies_states[goalie_0.KEY],                        \n",
    "                        strikers_states[STRIKER_1_KEY],                 \n",
    "                        goalies_states[GOALIE_1_KEY]                        \n",
    "                    ), axis=0 ),               \n",
    "                action_striker_0,\n",
    "                log_prob_striker_0,\n",
    "                striker_0_reward\n",
    "            )\n",
    "\n",
    "\n",
    "            # exit loop if episode finished\n",
    "            if done:\n",
    "                break  \n",
    "\n",
    "            # roll over states to next time step\n",
    "            goalies_states = goalies_next_states\n",
    "            strikers_states = strikers_next_states\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "        # learn\n",
    "        goalie_loss = goalie_optimizer.learn(goalie_0.memory)\n",
    "        striker_loss = striker_optimizer.learn(striker_0.memory)        \n",
    "\n",
    "        goalie_actor_model.checkpoint( CHECKPOINT_GOALIE_ACTOR )   \n",
    "        goalie_critic_model.checkpoint( CHECKPOINT_GOALIE_CRITIC )    \n",
    "        striker_actor_model.checkpoint( CHECKPOINT_STRIKER_ACTOR )    \n",
    "        striker_critic_model.checkpoint( CHECKPOINT_STRIKER_CRITIC )\n",
    "\n",
    "        team_0_score = goalies_scores[goalie_0.KEY] + strikers_scores[striker_0.KEY]\n",
    "        team_0_window_score.append( team_0_score )\n",
    "        team_0_window_score_wins.append( 1 if team_0_score > 0 else 0)        \n",
    "\n",
    "        team_1_score = goalies_scores[GOALIE_1_KEY] + strikers_scores[STRIKER_1_KEY]\n",
    "        team_1_window_score.append( team_1_score )\n",
    "        team_1_window_score_wins.append( 1 if team_1_score > 0 else 0 )\n",
    "\n",
    "        draws.append( team_0_score == team_1_score )\n",
    "        \n",
    "        print('Episode: {} \\tSteps: \\t{} \\tGoalie Loss: \\t {:.10f} \\tStriker Loss: \\t {:.10f}'.format( episode + 1, steps, goalie_loss, striker_loss ))\n",
    "        print('\\tRed Wins: \\t{} \\tScore: \\t{:.5f} \\tAvg: \\t{:.2f}'.format( np.count_nonzero(team_0_window_score_wins), team_0_score, np.sum(team_0_window_score) ))\n",
    "        print('\\tBlue Wins: \\t{} \\tScore: \\t{:.5f} \\tAvg: \\t{:.2f}'.format( np.count_nonzero(team_1_window_score_wins), team_1_score, np.sum(team_1_window_score) ))\n",
    "        print('\\tDraws: \\t{}'.format( np.count_nonzero(draws) ))\n",
    "\n",
    "        if np.count_nonzero( team_0_window_score_wins ) >= 95:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppo_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "\tRed Wins: \t1 \tScore: \t1.10000 \tAvg: \t1.10\n",
      "\tBlue Wins: \t0 \tScore: \t-1.10000 \tAvg: \t-1.10\n",
      "\tDraws: \t0\n",
      "Episode 2\n",
      "\tRed Wins: \t2 \tScore: \t1.10000 \tAvg: \t2.20\n",
      "\tBlue Wins: \t0 \tScore: \t-1.10000 \tAvg: \t-2.20\n",
      "\tDraws: \t0\n",
      "Episode 3\n",
      "\tRed Wins: \t2 \tScore: \t-1.10000 \tAvg: \t1.10\n",
      "\tBlue Wins: \t1 \tScore: \t1.10000 \tAvg: \t-1.10\n",
      "\tDraws: \t0\n",
      "Episode 4\n",
      "\tRed Wins: \t3 \tScore: \t1.10000 \tAvg: \t2.20\n",
      "\tBlue Wins: \t1 \tScore: \t-1.10000 \tAvg: \t-2.20\n",
      "\tDraws: \t0\n",
      "Episode 5\n",
      "\tRed Wins: \t3 \tScore: \t-1.10000 \tAvg: \t1.10\n",
      "\tBlue Wins: \t2 \tScore: \t1.10000 \tAvg: \t-1.10\n",
      "\tDraws: \t0\n",
      "Episode 6\n",
      "\tRed Wins: \t4 \tScore: \t1.10000 \tAvg: \t2.20\n",
      "\tBlue Wins: \t2 \tScore: \t-1.10000 \tAvg: \t-2.20\n",
      "\tDraws: \t0\n",
      "Episode 7\n",
      "\tRed Wins: \t4 \tScore: \t-1.10000 \tAvg: \t1.10\n",
      "\tBlue Wins: \t3 \tScore: \t1.10000 \tAvg: \t-1.10\n",
      "\tDraws: \t0\n",
      "Episode 8\n",
      "\tRed Wins: \t4 \tScore: \t-1.10000 \tAvg: \t-0.00\n",
      "\tBlue Wins: \t4 \tScore: \t1.10000 \tAvg: \t0.00\n",
      "\tDraws: \t0\n",
      "Episode 9\n",
      "\tRed Wins: \t5 \tScore: \t1.10000 \tAvg: \t1.10\n",
      "\tBlue Wins: \t4 \tScore: \t-1.10000 \tAvg: \t-1.10\n",
      "\tDraws: \t0\n",
      "Episode 10\n",
      "\tRed Wins: \t6 \tScore: \t1.10000 \tAvg: \t2.20\n",
      "\tBlue Wins: \t4 \tScore: \t-1.10000 \tAvg: \t-2.20\n",
      "\tDraws: \t0\n",
      "Episode 11\n",
      "\tRed Wins: \t7 \tScore: \t1.10000 \tAvg: \t3.30\n",
      "\tBlue Wins: \t4 \tScore: \t-1.10000 \tAvg: \t-3.30\n",
      "\tDraws: \t0\n",
      "Episode 12\n",
      "\tRed Wins: \t8 \tScore: \t1.10000 \tAvg: \t4.40\n",
      "\tBlue Wins: \t4 \tScore: \t-1.10000 \tAvg: \t-4.40\n",
      "\tDraws: \t0\n",
      "Episode 13\n",
      "\tRed Wins: \t8 \tScore: \t-1.10000 \tAvg: \t3.30\n",
      "\tBlue Wins: \t5 \tScore: \t1.10000 \tAvg: \t-3.30\n",
      "\tDraws: \t0\n",
      "Episode 14\n",
      "\tRed Wins: \t9 \tScore: \t1.10000 \tAvg: \t4.40\n",
      "\tBlue Wins: \t5 \tScore: \t-1.10000 \tAvg: \t-4.40\n",
      "\tDraws: \t0\n",
      "Episode 15\n",
      "\tRed Wins: \t9 \tScore: \t-1.10000 \tAvg: \t3.30\n",
      "\tBlue Wins: \t6 \tScore: \t1.10000 \tAvg: \t-3.30\n",
      "\tDraws: \t0\n",
      "Episode 16\n",
      "\tRed Wins: \t10 \tScore: \t1.10000 \tAvg: \t4.40\n",
      "\tBlue Wins: \t6 \tScore: \t-1.10000 \tAvg: \t-4.40\n",
      "\tDraws: \t0\n",
      "Episode 17\n",
      "\tRed Wins: \t11 \tScore: \t1.10000 \tAvg: \t5.50\n",
      "\tBlue Wins: \t6 \tScore: \t-1.10000 \tAvg: \t-5.50\n",
      "\tDraws: \t0\n",
      "Episode 18\n",
      "\tRed Wins: \t12 \tScore: \t1.10000 \tAvg: \t6.60\n",
      "\tBlue Wins: \t6 \tScore: \t-1.10000 \tAvg: \t-6.60\n",
      "\tDraws: \t0\n",
      "Episode 19\n",
      "\tRed Wins: \t13 \tScore: \t1.10000 \tAvg: \t7.70\n",
      "\tBlue Wins: \t6 \tScore: \t-1.10000 \tAvg: \t-7.70\n",
      "\tDraws: \t0\n",
      "Episode 20\n",
      "\tRed Wins: \t14 \tScore: \t1.10000 \tAvg: \t8.80\n",
      "\tBlue Wins: \t6 \tScore: \t-1.10000 \tAvg: \t-8.80\n",
      "\tDraws: \t0\n"
     ]
    }
   ],
   "source": [
    "# test the trained agents\n",
    "team_0_window_score = deque(maxlen=100)\n",
    "team_0_window_score_wins = deque(maxlen=100)\n",
    "\n",
    "team_1_window_score = deque(maxlen=100)\n",
    "team_1_window_score_wins = deque(maxlen=100)\n",
    "\n",
    "draws = deque(maxlen=100)\n",
    "\n",
    "for episode in range(20):                                               # play game for n episodes\n",
    "    env_info = env.reset(train_mode=False)                              # reset the environment    \n",
    "    goalies_states = env_info[g_brain_name].vector_observations         # get initial state (goalies)\n",
    "    strikers_states = env_info[s_brain_name].vector_observations        # get initial state (strikers)\n",
    "\n",
    "    goalies_scores = np.zeros(n_goalie_agents)                          # initialize the score (goalies)\n",
    "    strikers_scores = np.zeros(n_striker_agents)                        # initialize the score (strikers)\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        # select actions and send to environment\n",
    "        action_goalie_0, log_prob_goalie_0 = goalie_0.act( goalies_states[goalie_0.KEY] )\n",
    "        action_striker_0, log_prob_striker_0 = striker_0.act( strikers_states[striker_0.KEY] )\n",
    "\n",
    "        action_goalie_1, log_prob_goalie_1 = goalie_1.act( goalies_states[goalie_1.KEY] )\n",
    "        action_striker_1, log_prob_striker_1 = striker_1.act( strikers_states[striker_1.KEY] )\n",
    "        \n",
    "        # random            \n",
    "        #action_goalie_1 = np.asarray( [np.random.randint(goalie_action_size)] )\n",
    "        #action_striker_1 = np.asarray( [np.random.randint(striker_action_size)] )\n",
    "\n",
    "\n",
    "        actions_goalies = np.array( (action_goalie_0, action_goalie_1) )                                    \n",
    "        actions_strikers = np.array( (action_striker_0, action_striker_1) )\n",
    "\n",
    "        actions = dict( zip( [g_brain_name, s_brain_name], [actions_goalies, actions_strikers] ) )\n",
    "\n",
    "    \n",
    "        env_info = env.step(actions)                                                \n",
    "        # get next states\n",
    "        goalies_next_states = env_info[g_brain_name].vector_observations         \n",
    "        strikers_next_states = env_info[s_brain_name].vector_observations\n",
    "        \n",
    "        # get reward and update scores\n",
    "        goalies_rewards = env_info[g_brain_name].rewards  \n",
    "        strikers_rewards = env_info[s_brain_name].rewards\n",
    "        goalies_scores += goalies_rewards\n",
    "        strikers_scores += strikers_rewards\n",
    "                    \n",
    "        # check if episode finished\n",
    "        done = np.any(env_info[g_brain_name].local_done)\n",
    "\n",
    "        # exit loop if episode finished\n",
    "        if done:\n",
    "            break  \n",
    "\n",
    "        # roll over states to next time step\n",
    "        goalies_states = goalies_next_states\n",
    "        strikers_states = strikers_next_states\n",
    "\n",
    "        steps += 1\n",
    "        \n",
    "    team_0_score = goalies_scores[goalie_0.KEY] + strikers_scores[striker_0.KEY]\n",
    "    team_0_window_score.append( team_0_score )\n",
    "    team_0_window_score_wins.append( 1 if team_0_score > 0 else 0)        \n",
    "\n",
    "    team_1_score = goalies_scores[GOALIE_1_KEY] + strikers_scores[STRIKER_1_KEY]\n",
    "    team_1_window_score.append( team_1_score )\n",
    "    team_1_window_score_wins.append( 1 if team_1_score > 0 else 0 )\n",
    "\n",
    "    draws.append( team_0_score == team_1_score )\n",
    "    \n",
    "    print('Episode {}'.format( episode + 1 ))\n",
    "    print('\\tRed Wins: \\t{} \\tScore: \\t{:.5f} \\tAvg: \\t{:.2f}'.format( np.count_nonzero(team_0_window_score_wins), team_0_score, np.sum(team_0_window_score) ))\n",
    "    print('\\tBlue Wins: \\t{} \\tScore: \\t{:.5f} \\tAvg: \\t{:.2f}'.format( np.count_nonzero(team_1_window_score_wins), team_1_score, np.sum(team_1_window_score) ))\n",
    "    print('\\tDraws: \\t{}'.format( np.count_nonzero( draws ) ))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
